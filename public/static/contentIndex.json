{"fast_livo_2":{"slug":"fast_livo_2","filePath":"fast_livo_2.md","title":"fast_livo_2","links":[],"tags":[],"content":"Abstract\nThe author condensed the abstract into the following 4 aspects:\nQ1: What is Fast-LIVO2?\nFAST-LIVO2 is a real-time, accurate, and robust tightly coupled multi-sensor SLAM system. It effectively integrates heterogeneous sensor data from lidar, inertial, and visual sensors using the Error-State Iterated Kalman Filter (ESIKF).\n(Note: The lidar sensors include repetitive scanning multi-line rotating lidars such as Velodyne-16, XT-32, Ouster-64, Pandar128, and non-repetitive scanning solid-state lidars such as Livox-Avia. Cameras include pinhole cameras and fisheye cameras.)\nQ2: What does FAST-LIVO2 solve?\n\nHeterogeneity of measurement data between LIDAR and camera;\nSystem Efficiency\nSystem accuracy\nSystem Robustness\n\nQ3: What does FAST-LIVO2 do?\n\nEmploy a sequential update strategy (first running LIO then running VIO based on the state updated by LIO) to effectively fuse heterogenous measurements from LiDAR and camera sensors into a unified system.\nIt Directly registers raw data, avoiding complex and time consuming feature extraction processes. Measurements from both LiDAR and camera sensors are integrated into a unified voxel map (octree), enabling efficient management of 3D data.\nEstablishes associations between LiDAR points and image patches to achieve accurate image alignment. Additionally, it dynamically refines plane parameters corresponding to LiDAR points, further enhancing alignment accuracy.\nEmploys on demand voxel ray casting to overcome situations where data lies within LiDAR blind spots. Additionally camera eposure times are estimated online to address timestamp offsets caused by variable exposure durations under uneven illumination conditions.\n\nQ4: What has FAST-LIVO2 demonstrated?\n\nReal-time performance (efficiency): Demonstrated through UAV navigation experiments.\nAccuracy and robustness: Proven by UAV-based 3D reconstruction.\nScalability: Output results can be directly utilized for rendering methods like Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3D-GS).\n\nIntroduction\nThe introduction can be distilled into four sections:\n1. The Significance of SLAM:\nIntroduce application examples and highlight the potential of SLAM.\n2. Issues in SLAM (Motivation for FAST-LIVO2):\nBegin by discussing single-sensor limitations, leading to multi-sensor integration needs, and ultimately introducing FAST-LIVO2.\n\nSingle-sensor SLAM:\n\na. Visual SLAM: Rich texture information and strong potential for scene understanding; however, it lacks depth information and struggles with textureless areas, uneven illumination, and noise.\nb. LiDAR SLAM: Provides precise depth information but lacks texture information and struggles with structured environments (e.g., tunnels or planar walls).\n\n\nNecessity for Multi-sensor SLAM (LiDAR-Inertial-Visual SLAM or LIVO):\nAchieves more accurate state estimation, denser and texture-rich 3D maps, and greater environmental adaptability—even if individual sensors degrade.\nCurrent challenges faced by LIVO:\n\na. Low computational efficiency due to massive point clouds within limited onboard computing resources.\nb. Feature extraction-based methods reduce computational load but involve extensive engineering tricks and struggle in textureless or structureless environments.\nc. Unified map management can significantly improve system efficiency, but designing a data structure that effectively integrates heterogeneous measurements from LiDAR and cameras is challenging.\nd. Generating precise texture maps requires pixel-level accuracy, demanding highly accurate hardware synchronization, precise sensor extrinsics, sensor noise analysis, and efficient algorithmic design.\n\n\n\n3. The Concept and Significance of FAST-LIVO2:\nThe aforementioned points form both the problems and motivation. FAST-LIVO2 uses an Error-State Iterated Kalman Filter (ESIKF) to sequentially update states for LiDAR and visual modules:\n\nIMU measurements are used for state prediction in ESIKF, providing a prior state.\nBased on this prior, LiDAR point clouds are undistorted through back-propagation, computing the LiDAR measurement equations (plane-to-point distances). The state is iteratively updated through ESIKF, followed by updates to a voxel octree map (plane priors), obtaining a quasi-posterior state.\nUsing this quasi-posterior state, LiDAR points serve as visual map points. The photometric errors between reference image patches and the projection of visual map points onto the current image are computed, followed by ESIKF iterative updates, exposure time estimation, and updating reference patches, ultimately obtaining the posterior state.\nAdditionally, FAST-LIVO2 employs on-demand voxel ray-casting (note: detailed explanation follows later) to address LiDAR blind spots.\nContributions:\n\n\nProposes a sequential-update ESIKF framework integrating heterogeneous measurements, improving system robustness.\nRefines LiDAR-derived plane priors (plane parameters) to enhance precision, avoiding assumptions that image patches have uniform depth (note: if unclear, the core difference lies in whether an affine transform is computed based on plane parameters; detailed explanations can be found in code analysis).\nIntroduces a reference-patch updating strategy, improving image alignment accuracy.\nEstimates camera exposure time online to handle varying textures.\nUses on-demand voxel ray-casting to enhance robustness in areas lacking LiDAR points.\n\nRelated Work\nThe related work can be summarized into three parts:\n1. Overview of Direct Methods in SLAM\n2. Introduction to LIVO\n3. Development Timeline of LIVO (Note: The timeline is based solely on works mentioned in this paper. Given the author’s limitations, some relevant work may inevitably be overlooked.)\n1. Overview of Direct Methods\nA comparative analysis between feature-based and direct methods is presented.\nClassification of Visual Direct Methods:\n\nDense Direct Methods (all pixels):\nTypically applied to RGB-D cameras. Examples include:\n\n“Real-time dense visual tracking under large lighting variations”\n“Direct iterative closest point for real-time visual odometry”\n“Robust odometry estimation for RGB-D cameras”\n\n\nSemi-dense Direct Methods (pixels with significant grayscale gradients):\nExamples include:\n\n“LSD-SLAM: Large-scale direct monocular SLAM”\n“Semi-dense visual odometry for a monocular camera”\n\n\nSparse Direct Methods (image patches):\nBased on carefully selected image patches. Examples include:\n\n“Direct sparse odometry”\n“SVO: Fast semi-direct monocular visual odometry”\n\n\n\nDirect Methods in LiDAR:\nThere is no explicit categorization. Typically involves downsampling scans and establishing point-to-plane constraints. Representative works include:\n\n“FAST-LIO2: Fast direct lidar-inertial odometry”\n“Efficient and probabilistic adaptive voxel mapping for accurate online lidar odometry”\n“Direct lidar-inertial odometry: Lightweight LIO with continuous-time motion correction”\n“D-LIOM: Tightly-coupled direct lidar-inertial odometry and mapping”\nDirect Methods in FAST-LIVO2:\nFor the LiDAR module: utilizes methods from “Efficient and probabilistic adaptive voxel mapping for accurate online lidar odometry” (Note: manages voxel maps using hash tables and octrees).\nFor the visual module: adopts “SVO: Fast semi-direct monocular visual odometry” (Note: employs a sparse direct method to compute the Jacobian matrix of photometric error relative to pose perturbations. FAST-LIVO2 differs from SVO in terms of coordinate systems and thus requires an additional processing step—details available in code analysis).\n\n2. Introduction to Multi-sensor Fusion SLAM\n(Note: This section is highly recommended for quickly grasping relevant LIVO research.)\nThe authors classify existing LIVO methods into two categories: loosely-coupled and tightly-coupled. The classification is mainly based on two criteria:\n\n\nState Estimation:\nWhether the estimation result from one sensor is included as an optimization target within another sensor’s model.\n\n\nRaw Measurement Integration:\nWhether raw data from different sensors are directly combined.\n\n\n3. Development Timeline of LIVO\nFirst, according to the classification standards in Section 2, all existing LIVO methods are categorized as shown below:\n\nFigure 1: LIVO Development Timeline\nLoosely-coupled Methods: Discussed from the aspects of states and measurements, illustrated as follows:\n\nFigure 2: Timeline of Loosely-coupled Methods (not included here, referenced from the original text).\nTightly-coupled Methods: The paper discusses these methods from two perspectives: indirect methods and direct methods.\n\nSystem Overview\nThis section primarily describes the processing flow of the entire system.\nFigure 4 illustrates the system architecture, showing that the system consists of four core modules:\n\nSequential-update ESIKF\nLocal map construction\nLiDAR observation model\nVisual observation model\n\nSpecifically, Module 1 deals with the ESIKF’s state prediction/update equations, while Modules 3 and 4 define the state observation equations of the ESIKF.\nThe processing flow is as follows:\nLiDAR points sampled asynchronously are merged to align with the camera’s sampling timestamps → Sequential updates are performed using ESIKF (first LiDAR, then camera). Both sensor modalities utilize a voxel map for management.\nThe LiDAR observation model performs frame-to-map point-to-plane residual calculations.\nVisual map points within the current Field of View (FoV) are extracted from the voxel map using visible voxel queries and on-demand ray casting. Photometric errors (frame-to-map residuals, meaning the variation of photometric error between current and reference frames under pose perturbation ^GT_I) are computed, enabling visual updates.\nLocal map maintenance: LiDAR-Inertial Odometry (LIO) updates geometric information of the voxel map, whereas Visual-Inertial Odometry (VIO) updates the current and reference image patches associated with the visual map points. (Updated reference image patches are further refined to obtain their normals in a separate thread.)\nSequential-update ESIKF\nf readers are already familiar with ESIKF or the FAST-LIO system, this section can be skipped. However, for those unfamiliar, this section provides a step-by-step introduction to ESIKF.\nThis section introduces the mathematical models for state prediction and update, structured into four subsections:\nS\n\nNotation and State Transition Model\nScan Merging\nPropagation (Forward and Backward Propagation)\nSequential Updates\n\nNotation and State Transition Model\nFirst, sensor coordinate frames are defined:\n\\begin{aligned} \nI : \\text{IMU frame} \\\\ \nG : \\text{Global frame} \n\\end{aligned}\n(Note: Only IMU and global frames are defined here since the entire system state is IMU-centric. Observations from LiDAR and camera are transformed into the IMU frame.)\nNext, the discrete-time state transition equation is given by:\nx_{i+1} = x_i \\boxplus (\\Delta t f(x_i, u_i, w_i))\n\nx_i is the state vector\nu_i represents IMU observations\nw_i is the process noise\n\\Delta t is the IMU sampling interval\nf is the kinematic equation matrix\n(Note: When deriving the state-space equations, some methods begin by differentiating the kinematic equations in continuous time to obtain continuous-time state-space equations, which are then discretized. Discretization typically involves either Euler integration or midpoint integration. This process can be tedious, as continuous-time equations must first be obtained. A more convenient alternative is to directly discretize the kinematic equations and derive discrete-time state-space equations directly. Readers are encouraged to refer to additional blogs for further details on state-space equation derivations.)\n\nExplanation of the \\boxplus operator:\nReaders who have only read the FAST-LIVO2 paper may wonder what the \\boxplus symbol represents. In fact, it’s not complicated. This operator is explained clearly in the paper “FAST-LIO: A Fast, Robust LiDAR-Inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter.” The operator arises primarily because rotation matrices R\\in SO(3) do not form a closed group under standard addition. Simply put, consider a coordinate frame F undergoing rotations R^0_{1}​ followed by R^1_2​, resulting in rotation R^0_2​. In this case, the relationship is R^0_2\\neq R^0_1 + R^1_2​, but rather R^0_2=R^0_1R^1_2​. Thus, the standard addition operator (+) becomes inappropriate for representing rotation increments. Hence, the \\boxplus operator is introduced to suitably represent “addition” operations involving both the rotation space SO(3) and the Euclidean space \\mathbb{R}^n.\n(Note: This explanation is a simplified interpretation. To thoroughly understand addition and subtraction operations in SO(3), readers are encouraged to further explore concepts related to manifolds and Rodrigues’ formula in sources like “Autonomous Driving and SLAM Technology in Robotics” or “14 Lectures on Visual SLAM” by Gao Xiang.)"},"index":{"slug":"index","filePath":"index.md","title":"learnslam.info","links":["fast_livo_2"],"tags":[],"content":"The goal of this website is to allow a reader with minimal math and cs background to fully* learn how state of the art SLAM systems work, primarily focusing on FAST-LIVO2.\nMost notes and explanations will be translated and based on the repo FAST_LIVO2_Noted by Pan Feng. Many thanks for his herculean effort in annotating the paper, code, and hardware setup.\nBegin here:\nFast LIVO2 Paper Explained"}}